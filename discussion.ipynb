{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical assignment for LAMAlab candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data and relevant imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "with open(\"data.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "y = np.array(data[\"y\"])\n",
    "X = [np.array(point_cloud) for point_cloud in data[\"X\"]]\n",
    "assert len(X) == len(y)\n",
    "assert isinstance(X,  list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary considerations\n",
    "\n",
    "* The goal is to predict a scalar quantity (energy) of a 3D point cloud (xyz positions)\n",
    "* This scalar quanitity is invariant to the translation and rotations of 3D point cloud, and it should be permutationally invariant with respect to the order of points inside the cloud.\n",
    "* The points are not annotated: the only feature each point has is it's position\n",
    "* We will assume the energy depends on the interaction between points, which depends on their relative positions, but we don't assume anything about the nature of the interactions (one-body, two-body, 3-body terms etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A general discussion how to incorporate invariances into the model\n",
    "1. The simplest approach is to train a model that doesn't have any invariances encoded in its architecture. Instead, we augment the training data with rotated, translated and permutated examples from the original dataset and hope that the model will learn these invariances. However, this approach fails for multiple reasons:\n",
    "    * This is highly inpracical, as there are infinitely many rotations and translations that can be used, and n! permutations among n points\n",
    "    * The model will only be approximately invariant, even if we could augment the dataset with all possible transformations, because the training never converges to the true global minimum\n",
    "    * Even if we could find the global minimum, most of the models can't be rotationally invariant due to their architecture\n",
    "\n",
    "2. Transform the data based on some heuristic before passing it to the model. For instance, we could always translate the point cloud so that the geometric center is located at the origin of coordinate system. However, it's not clear how to do the similar thing for rotation. Some models are proposed to solve this task, either as self-supervised models or as a part of the larger architecture.\n",
    "    * The advantage of this approach is the flexibility and expressivity of the model can be better if the model is not contrained by invariances.\n",
    "    * Downsides are the artifacts model can learn based on the pretransforming heuristic and there is no clear way to achive true rotational invariance.\n",
    "\n",
    "3. Transform the features into rotationally and translationally invariant features. In this example, we don't have any features other than the positions themselves. Therefore, instead of having the matrix of points positions, we can work with pairwase distance matrix, which is rotationally and translationaly invariant. However, we lose some infomation about angles using this approach, because mapping from the positions to pairwase distances is not unique. It might be benefitial to have more than one scalar value per atom pair instead. In this case, we can expand the distance inside a basis set. A popular choice is set of radial basis functions. i-th element of this encoding is defined as:\n",
    "$$\n",
    "e_i = exp(-(r-d_i)^2/ \\alpha)\n",
    "$$\n",
    "where $\\alpha$ is a hyperparameter and $d_i$ is a distance on a predefined grid.\n",
    "\n",
    "4. To address permutational invarinace, it's enough to use some reduction along points axis, like a sum, mean, max. Some learnable permutationally invariant function can also work. This is the core idea behind graph neural networks, where permutational invariance is achieved using permutationally invariant aggregation function.\n",
    "\n",
    "5. Another approach is to use equivariant layers in the model architecture, followed by reduction that makes it invariant. This is how the current SOTA models are deisgned. In short, we expand the point clouds in spherical harmonics basis, and then the layers are complex matematical functions that keep rotational and translational eqivariance. Output from those layers are vectors equivariant to rotation and translation. We can later reduce the vectors for each point into a single vector, making the whole model invariant to rotation, translation and permutation. This approach also works well for predicting energies, as it might be desirable to get the contribution of each point towards the results. A natural choice in case of energy is to use sum.\n",
    "\n",
    "6. Finally, we can use already existing well-tested models like SchNet, PaiNN or similar models, without the need to implement any layer ourselves. These models are still based on the principles discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch import Trainer, LightningDataModule, LightningModule\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "class list_dataset(Dataset):\n",
    "    def __init__(self, data, targets) -> None:\n",
    "        super().__init__()\n",
    "        self.data = [torch.tensor(example, dtype=torch.float32) for example in data]\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32).view(-1,1)\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index) -> tuple:\n",
    "        return (self.data[index], self.targets[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance_matrix\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "def from_coords_to_distances(r: np.ndarray):\n",
    "    return distance_matrix(r,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = X[0]\n",
    "rotation = R.from_euler(\"xyz\",angles=[45,46,7],degrees=True)\n",
    "rotated_example = rotation.apply(X[0])\n",
    "translated_example = example + np.array([5,6,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the average pairwise distance is rotationally invariant\n",
    "np.allclose(from_coords_to_distances(example), from_coords_to_distances(rotated_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the average pairwise distance is translationally invariant\n",
    "np.allclose(from_coords_to_distances(example), from_coords_to_distances(translated_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we still didn't achieve permutational invariance. In order to do that, we can do a simple reduction over point axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367.12981471716637\n",
      "367.12981471716637\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng()\n",
    "permuted_example = example[rng.permutation(example.shape[0]),:]\n",
    "print(np.sum(from_coords_to_distances(example),axis=(0,1)))\n",
    "print(np.sum(from_coords_to_distances(permuted_example),axis=(0,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to achieve rotational invariance, we had to lower our input feature dimensions from NxNx3 to NxN. To get permutational invariance we had to lower the dimensionality to 1. However, trying to predict a scalar from a single feature is not going to work, so we have to move to GNN framework to get permutational invariance and to use more sofisticated ways to get rotationally invariant atomic (or edge) features. It's time to move to more advanced atomic descriptors that are rotationally and translationally invariant.\n",
    "### SOAP descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dscribe.descriptors import SOAP\n",
    "from ase import Atoms\n",
    "species = [\"H\"]\n",
    "r_cut = 6.0 # distance cutoff\n",
    "n_max = 8 # number of radial basis functions\n",
    "l_max = 6 # number of spherical harmonics\n",
    "\n",
    "# Setting up the SOAP descriptor\n",
    "soap = SOAP(\n",
    "    species=species,\n",
    "    periodic=False,\n",
    "    r_cut=r_cut,\n",
    "    n_max=n_max,\n",
    "    l_max=l_max,\n",
    ")\n",
    "dim = soap.get_number_of_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our point cloud to ase atoms object\n",
    "def cloud_to_ase(x: np.ndarray)-> Atoms:\n",
    "    return Atoms(symbols=[\"H\"] * x.shape[0],positions=x)\n",
    "\n",
    "X_soap = soap.create([cloud_to_ase(x) for x in X ], n_jobs=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = soap.create(cloud_to_ase(X[0]))\n",
    "rotation = R.from_euler(\"xyz\",angles=[45,46,7],degrees=True)\n",
    "rotated_example = soap.create(cloud_to_ase(rotation.apply(X[0]))) \n",
    "translated_example = soap.create(cloud_to_ase(X[0] + np.array([5,6,7])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.70896060e-02,  8.40539773e-02,  2.06599738e-01, ...,\n",
       "         3.76073734e-01, -3.11124842e-01,  2.58194933e-01],\n",
       "       [ 1.71178016e-02,  8.40629731e-02,  2.06935492e-01, ...,\n",
       "         3.03293363e-01, -2.47287011e-01,  2.02978732e-01],\n",
       "       [ 1.73409536e-02,  8.41256937e-02,  2.10212706e-01, ...,\n",
       "         9.91709154e-11, -3.88799513e-11,  1.52428825e-11],\n",
       "       [ 1.73427907e-02,  8.41256108e-02,  2.10243665e-01, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 1.72801489e-02,  8.41297401e-02,  2.09158197e-01, ...,\n",
       "         3.20626788e-05, -1.46836198e-05,  6.72460494e-06],\n",
       "       [ 1.70725476e-02,  8.40572568e-02,  2.06143088e-01, ...,\n",
       "         1.96111897e-01, -1.41992399e-01,  1.03082929e-01]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for rotational invariance\n",
    "np.allclose(example,rotated_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datamodule(LightningDataModule):\n",
    "    def __init__(self, X: list[np.ndarray],y: np.ndarray, batch_size: int = 1) -> None:\n",
    "        super().__init__()\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X,y, test_size=0.1, shuffle=True, random_state=42)\n",
    "        self.batch_size = batch_size\n",
    "    def setup(self, stage: str) -> None:\n",
    "        self.train_data = list_dataset(self.X_train, self.y_train)\n",
    "        self.test_data = list_dataset(self.X_test, self.y_test)\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.train_data, batch_size=self.batch_size, num_workers=7, shuffle=True)\n",
    "    \n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.test_data, batch_size=self.batch_size, num_workers=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "class SOAP_FFN(LightningModule):\n",
    "    def __init__(self, dim: int = 252, hidden_size: int = 256, num_hidden: int = 3, lr: float = 3e-4) -> None:\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(dim,hidden_size), nn.ReLU()]\n",
    "        for _ in range(num_hidden):\n",
    "            layers.extend([nn.Linear(hidden_size,hidden_size), nn.ReLU()])\n",
    "        layers.append(nn.Linear(hidden_size,1))\n",
    "        self.ffn = nn.Sequential(*layers)\n",
    "        self.lr = lr\n",
    "    def forward(self, x: torch.Tensor) -> Any:\n",
    "        return torch.sum(self.ffn(x),axis=1)\n",
    "    \n",
    "    def training_step(self, batch) -> Any:\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = F.mse_loss(y_hat.squeeze(),y.squeeze())\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch) -> Any:\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        test_loss = F.mse_loss(y_hat.squeeze(),y.squeeze())\n",
    "        self.log(\"test_loss\", test_loss, prog_bar=True,on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self) -> torch.optim.Optimizer:\n",
    "        adam = torch.optim.Adam(self.parameters(),lr= self.lr)\n",
    "        return adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2024-04-26 19:31:36.128279: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-26 19:31:36.128381: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-26 19:31:36.165913: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-26 19:31:36.255827: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-26 19:31:37.400190: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\n",
      "  | Name | Type       | Params\n",
      "------------------------------------\n",
      "0 | ffn  | Sequential | 196 K \n",
      "------------------------------------\n",
      "196 K     Trainable params\n",
      "0         Non-trainable params\n",
      "196 K     Total params\n",
      "0.786     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d78f3884cd584a6a9c3263dd1fc827f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d2453d4cd84bd49db274b4e08a3240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.03297505900263786    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.03297505900263786   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.03297505900263786}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(max_epochs=5)\n",
    "model = SOAP_FFN(dim,hidden_size=256,num_hidden=2)\n",
    "datamodule = Datamodule(X_soap,y)\n",
    "trainer.fit(model,datamodule)\n",
    "trainer.test(model,datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometric deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The downside of any atomic descriptor is that it's not trainable. Some problems might require different transformations of xyz to invariant coordinates in order to achieve good performance with less parameters. Here I will use SchNet model, the model used as the baseline to compare performance of any new geometric NN for quantum chemistry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from numpy import ndarray\n",
    "from torch_geometric.nn.models import SchNet\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "from torch_geometric.data import Data\n",
    "class SchNetModule(LightningModule):\n",
    "    def __init__(self,lr, *args: Any, **kwargs: Any) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.model = SchNet(*args)\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, batch) -> Any:\n",
    "        atom_nums = torch.ones(batch.x.shape[0], dtype=torch.int32)\n",
    "        return self.model.forward(atom_nums,batch.x,batch.batch)\n",
    "    \n",
    "    def training_step(self, batch) -> Any:\n",
    "        y_hat = self.forward(batch)\n",
    "        loss = F.mse_loss(y_hat.squeeze(),batch.y.squeeze())\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch) -> Any:\n",
    "        y_hat = self.forward(batch)\n",
    "        test_loss = F.mse_loss(y_hat.squeeze(),batch.y.squeeze())\n",
    "        self.log(\"test_loss\", test_loss, prog_bar=True,on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self) -> torch.optim.Optimizer:\n",
    "        adam = torch.optim.Adam(self.parameters(),lr= self.lr)\n",
    "        return adam\n",
    "    \n",
    "class PyGDataModule(LightningDataModule):\n",
    "    def __init__(self, X: list[np.ndarray],y: np.ndarray, batch_size: int = 1) -> None:\n",
    "        super().__init__()\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X,y, test_size=0.1, shuffle=True, random_state=42)\n",
    "        self.batch_size = batch_size\n",
    "    def setup(self, stage: str) -> None:\n",
    "        self.train_data = [Data(x=torch.tensor(x,dtype=torch.float32),y=torch.tensor(y,dtype=torch.float32)) for x,y in zip(self.X_train, self.y_train)]\n",
    "        self.test_data = [Data(x=torch.tensor(x,dtype=torch.float32),y=torch.tensor(y,dtype=torch.float32)) for x,y in zip(self.X_test, self.y_test)]\n",
    "    def train_dataloader(self) -> PyGDataLoader:\n",
    "        return PyGDataLoader(self.train_data, batch_size=self.batch_size, num_workers=1, shuffle=True)\n",
    "    \n",
    "    def test_dataloader(self) -> PyGDataLoader:\n",
    "        return PyGDataLoader(self.test_data, batch_size=self.batch_size, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | SchNet | 455 K \n",
      "---------------------------------\n",
      "455 K     Trainable params\n",
      "0         Non-trainable params\n",
      "455 K     Total params\n",
      "1.823     Total estimated model params size (MB)\n",
      "/home/zarko/miniconda3/envs/THESIS/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd337ef455164882935d4b53b76bfe66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n",
      "/home/zarko/miniconda3/envs/THESIS/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "993e866963214b65a448dc93278c3005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zarko/miniconda3/envs/THESIS/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 53. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/home/zarko/miniconda3/envs/THESIS/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 69. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/home/zarko/miniconda3/envs/THESIS/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 47. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/home/zarko/miniconda3/envs/THESIS/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 54. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/home/zarko/miniconda3/envs/THESIS/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 67. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/home/zarko/miniconda3/envs/THESIS/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 64. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/home/zarko/miniconda3/envs/THESIS/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 76. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/home/zarko/miniconda3/envs/THESIS/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 21. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/home/zarko/miniconda3/envs/THESIS/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 48. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/home/zarko/miniconda3/envs/THESIS/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 42. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/home/zarko/miniconda3/envs/THESIS/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 51. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/home/zarko/miniconda3/envs/THESIS/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 45. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/home/zarko/miniconda3/envs/THESIS/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 62. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/home/zarko/miniconda3/envs/THESIS/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 61. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/home/zarko/miniconda3/envs/THESIS/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 63. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/home/zarko/miniconda3/envs/THESIS/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 41. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.01617174781858921    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.01617174781858921   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.01617174781858921}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(max_epochs=5)\n",
    "model = SchNetModule(lr=3e-4)\n",
    "datamodule = PyGDataModule(X, y, batch_size=4)\n",
    "trainer.fit(model,datamodule)\n",
    "trainer.test(model,datamodule)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "THESIS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
